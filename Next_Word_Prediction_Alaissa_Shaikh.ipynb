{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **NEXT WORD PRDICTION USING DEEP LEARNING**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Project By: Alaissa Shaikh**\n",
        "### **Data Scientist**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jmweC7UAL1eB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project investigates the application of deep learning techniques to the challenging task of next word prediction. We explore the use of an LSTM network, a powerful architecture for sequential data, to model the complex relationships between words in a text sequence. The model is trained on a large dataset of text and demonstrates promising results in predicting the next word in a given context. This work contributes to the growing field of natural language processing and showcases the potential of deep learning for developing innovative language-based applications.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hLJ5ivlyMZ7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "eT66EzEoC387"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The library provide the necessary tools for data preprocessing, model architecture, and layer definitions.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "815i-suYHfJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Read the dataset\n",
        "with open('/content/sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "MsFCciGHC8no"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "wVBvvuLKDB6i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is crucial for subsequent steps in the next word prediction model, such as converting the text data into numerical sequences and determining the size of the vocabulary for the Embedding layer.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-vfE0iQbH3mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create input-output sequences\n",
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "SuRZXY47DDhs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterates through each line in the text data and creates input sequences for the next word prediction model. These input sequences consist of multiple words, which will be used to predict the next word in the sequence.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "t8rWjClDIA9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Pad the sequences\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ],
      "metadata": {
        "id": "a5VBEkdoDFOD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensures that all input sequences have the same length by padding shorter sequences with zeros at the beginning. This is necessary for many deep learning models, as they require input data to have a consistent shape.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ImoTQEt8IJhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Split data into features (X) and labels (y)\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]"
      ],
      "metadata": {
        "id": "m5waS-TdDIAD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will learn to predict the next word (label) given a sequence of words (features).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ioLeMzi6IT0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one-hot encoding\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
      ],
      "metadata": {
        "id": "aul0UNBFDKKl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converts the integer labels into a format suitable for training a neural network. One-hot encoding allows the model to easily learn the relationships between different words and their probabilities.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FIShBrYxIcMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100))  # Removed `input_length`\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))"
      ],
      "metadata": {
        "id": "EizNHK0RDMuZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines a simple but effective neural network architecture for the next word prediction task. The model takes input sequences of words, embeds them into dense vectors, processes them with an LSTM layer to capture sequential information, and finally outputs a probability distribution over the vocabulary, indicating the likelihood of each word being the next word in the sequence.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-3avVdxrIkhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "IYaENh4iDPcp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with dummy input\n",
        "model.build(input_shape=(None, max_sequence_len - 1))\n",
        "\n",
        "# Display model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "tWpUuSwcDqOK",
        "outputId": "368ddfcb-4e1c-4259-e9fb-fff1b21b6b7d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │         \u001b[38;5;34m820,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)                 │         \u001b[38;5;34m150,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8200\u001b[0m)                │       \u001b[38;5;34m1,238,200\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">820,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8200</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,238,200</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,208,800\u001b[0m (8.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,208,800</span> (8.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,208,800\u001b[0m (8.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,208,800</span> (8.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Total Parameters: The model has a total of 2,208,800 trainable parameters. This number indicates the complexity of the model and the amount of data required to train it effectively.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*  In summary: This model uses an embedding layer to represent words as vectors, an LSTM layer to capture sequential information, and a dense layer to generate a probability distribution over the vocabulary. The model is designed for next word prediction tasks and has a relatively large number of parameters, suggesting it may be a complex model.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BiZ80e-aLZRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Train the model\n",
        "model.fit(X, y, epochs=20, verbose=1)  # Reduce epochs for quicker training during testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSTh4cK8DtEi",
        "outputId": "18184143-d698-45e7-f733-3f4074d06d68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - accuracy: 0.0616 - loss: 6.5603\n",
            "Epoch 2/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 7ms/step - accuracy: 0.1177 - loss: 5.5789\n",
            "Epoch 3/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 7ms/step - accuracy: 0.1449 - loss: 5.1349\n",
            "Epoch 4/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.1650 - loss: 4.7807\n",
            "Epoch 5/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - accuracy: 0.1829 - loss: 4.4613\n",
            "Epoch 6/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.2056 - loss: 4.1656\n",
            "Epoch 7/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.2319 - loss: 3.8865\n",
            "Epoch 8/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - accuracy: 0.2643 - loss: 3.6165\n",
            "Epoch 9/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.2989 - loss: 3.3841\n",
            "Epoch 10/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.3299 - loss: 3.1604\n",
            "Epoch 11/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.3740 - loss: 2.9414\n",
            "Epoch 12/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - accuracy: 0.4049 - loss: 2.7535\n",
            "Epoch 13/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 7ms/step - accuracy: 0.4435 - loss: 2.5523\n",
            "Epoch 14/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.4747 - loss: 2.4035\n",
            "Epoch 15/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.5014 - loss: 2.2545\n",
            "Epoch 16/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 7ms/step - accuracy: 0.5328 - loss: 2.1126\n",
            "Epoch 17/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - accuracy: 0.5626 - loss: 1.9784\n",
            "Epoch 18/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.5851 - loss: 1.8638\n",
            "Epoch 19/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.6047 - loss: 1.7723\n",
            "Epoch 20/20\n",
            "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - accuracy: 0.6295 - loss: 1.6641\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7da947541b90>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate predictions\n",
        "def predict_next_words(seed_text, next_words, max_sequence_len, model, tokenizer):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, paddi# Generate predictions\n",
        "output_text = predict_next_words(seed_text, next_words, max_sequence_len, model, tokenizer)\n",
        "print(f\"Generated Text: {output_text}\")ng='pre')\n",
        "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "ml5QpIqMD45n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a seed text, a trained language model, and other parameters. It then iteratively predicts the next word, appends it to the seed text, and repeats the process to generate a sequence of words.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QhOYnrvhLqhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User input for seed text and number of words\n",
        "seed_text = input(\"Enter a seed text: \")\n",
        "next_words = int(input(\"Enter the number of words to predict: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0wtyu5dGG41",
        "outputId": "f4327133-a828-4929-90a7-cc2d5f7df16a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a seed text: sherlock holmes felt fine\n",
            "Enter the number of words to predict: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "output_text = predict_next_words(seed_text, next_words, max_sequence_len, model, tokenizer)\n",
        "print(f\"Generated Text: {output_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0U2eMF2GHf6",
        "outputId": "350ebac7-9b12-4283-ed64-ffcb2c7357c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Generated Text: sherlock holmes felt fine that i had a wild free of the victim back\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through this project, I gained valuable experience in implementing deep learning models for natural language processing tasks. I learned the importance of data preprocessing, model architecture design, and hyperparameter tuning. I also gained insights into the challenges and limitations of current language models. This project has provided a strong foundation for further exploration in the field of natural language processing and has inspired me to continue exploring the exciting possibilities of deep learning.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "l7ozUvG0MmuB"
      }
    }
  ]
}